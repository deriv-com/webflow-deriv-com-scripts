name: "Generate Sitemap.xml and Robots.txt"
description: "Generate Sitemap.xml and robots.txt for all domains and upload to R2"
inputs:
  main_domain:
    description: "Main domain for sitemap replacement"
    required: true
runs:
  using: "composite"
  steps:
    - name: Process and combine sitemaps
      shell: bash
      env:
        MAIN_DOMAIN: ${{ inputs.main_domain }}
      run: |
        mkdir -p content
        domain_name=$(echo $MAIN_DOMAIN | sed 's|https://||g')
        mkdir -p content/$domain_name

        # Process and combine sitemaps
        node .github/modify_sitemap_new.js \
          --staging-sitemap temp/staging_sitemap.xml \
          --academy-sitemap temp/academy_sitemap.xml \
          --output-file content/$domain_name/sitemap.xml \
          --new-domain $domain_name

        # Copy robots.txt and update it
        cp temp/robots.txt content/$domain_name/robots.txt

        # Generate robots.txt
        node .github/modify_robots.js \
          --sitemap-url $domain_name \
          --input-file content/$domain_name/robots.txt

        # Clean up temp files
        rm -rf temp
